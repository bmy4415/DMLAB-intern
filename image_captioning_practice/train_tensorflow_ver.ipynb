{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import gzip\n",
    "import random\n",
    "import time\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return image feature dict, caption dict\n",
    "def load_data(dtype):\n",
    "    if dtype == 'train':\n",
    "        filename = './Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "    elif dtype == 'valid':\n",
    "        filename = './Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "    elif dtype == 'test':\n",
    "        filename = './Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "        \n",
    "    image_ids = []\n",
    "    features = dict()\n",
    "    captions = dict()\n",
    "    \n",
    "    # get image ids\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        image_ids = [x.split('.')[0] for x in lines if x] # remove '.jpg'\n",
    "        \n",
    "    # get image features\n",
    "    with gzip.open('./image_features.pkl.zip', 'rb') as f:\n",
    "        all_featuers = pickle.load(f)\n",
    "        features = { x: all_featuers[x][0] for x in image_ids }\n",
    "        \n",
    "    # get image cpations\n",
    "    with open('./captions.txt', 'r') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        for l in lines:\n",
    "            tokens = l.split(' ')\n",
    "            image_id = tokens[0]\n",
    "            caption = ' '.join(tokens[1:])\n",
    "            if image_id in captions:\n",
    "                captions[image_id].append(caption)\n",
    "            else:\n",
    "                captions[image_id] = [caption]\n",
    "            \n",
    "            \n",
    "    return features, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of images in train: 6000\n",
      "# of images in valid: 1000\n",
      "# of images in test: 1000\n",
      "feature shape: (4096,)\n"
     ]
    }
   ],
   "source": [
    "train_features, train_captions = load_data('train')\n",
    "valid_features, valid_captions = load_data('valid')\n",
    "test_features, test_captions = load_data('test')\n",
    "\n",
    "print('# of images in train:', len(train_features))\n",
    "print('# of images in valid:', len(valid_features))\n",
    "print('# of images in test:', len(test_features))\n",
    "\n",
    "print('feature shape:', list(train_features.items())[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption에 있는 word 중 5번 이상 등장한 word만 사용하여\n",
    "# word_to_idx, idx_to_word dict를 만듬\n",
    "# word를 vector화 하여 input으로 주기 위함\n",
    "# return word_to_idx, idx_to_word, vocab_size\n",
    "\n",
    "def preprocess_vocab(captions):\n",
    "    threshold = 20 # threshold of occurence\n",
    "    count = dict()\n",
    "    word_to_idx, idx_to_word = dict(), list()\n",
    "\n",
    "    for caption_list in captions.values():\n",
    "        for c in caption_list:\n",
    "            tokens = c.split(' ')\n",
    "            for t in tokens:\n",
    "                if t in count:\n",
    "                    count[t] += 1\n",
    "                else:\n",
    "                    count[t] = 1\n",
    "            \n",
    "    print('# total of words:', len(count))\n",
    "    print('# of words that appears >= {}:'.format(threshold), len([w for w in count if count[w] >= threshold]))\n",
    "\n",
    "    idx_to_word = [w for w in count if count[w] >= threshold]\n",
    "    idx_to_word.append('<START>')\n",
    "    idx_to_word.append('<END>')\n",
    "    for i, w in enumerate(idx_to_word):\n",
    "        word_to_idx[w] = i\n",
    "        \n",
    "    return word_to_idx, idx_to_word, len(idx_to_word)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# total of words: 8764\n",
      "# of words that appears >= 20: 1289\n",
      "vocab_size: 1291\n"
     ]
    }
   ],
   "source": [
    "# vocab_size: 우리가 사용할 총 단어의 수\n",
    "# 각 caption을 index vector로 encoding할 수 있음\n",
    "word_to_idx, idx_to_word, vocab_size = preprocess_vocab(train_captions)\n",
    "print('vocab_size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 caption들을 숫자로 encoding함\n",
    "# caption의 최대길이를 구함\n",
    "# return encoded caption dict, max_len\n",
    "def encode_caption(word_to_idx, captions):\n",
    "    maxlen = 0\n",
    "    result = dict()\n",
    "    for image_id in captions:\n",
    "        caption_list = captions[image_id]\n",
    "        _caption_list = [] # encoded caption list\n",
    "        for c in caption_list:\n",
    "            tokens = c.split(' ')\n",
    "            embeded = []\n",
    "            for t in tokens:\n",
    "                # word_to_idx에 없는 단어는 무시\n",
    "                if t in word_to_idx:\n",
    "                    embeded.append(word_to_idx[t])\n",
    "\n",
    "            maxlen = max(maxlen, len(embeded))\n",
    "            embeded.insert(0, word_to_idx['<START>'])\n",
    "            embeded.append(word_to_idx['<END>'])\n",
    "            _caption_list.append(embeded)\n",
    "            \n",
    "        result[image_id] = _caption_list\n",
    "            \n",
    "    return result, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum length of encoded caption: 29\n",
      "embeded caption example\n",
      "('3524612244_64f00afec5', [[1289, 592, 558, 937, 453, 528, 544, 264, 505, 1290], [1289, 541, 454, 1227, 320, 916, 898, 684, 416, 454, 1194, 1290], [1289, 558, 937, 725, 439, 1290], [1289, 28, 528, 11, 583, 21, 898, 302, 937, 1290], [1289, 390, 403, 725, 1093, 772, 937, 544, 588, 1290]])\n"
     ]
    }
   ],
   "source": [
    "encoded_train_captions, maxlen = encode_caption(word_to_idx, train_captions)\n",
    "print('maximum length of encoded caption:', maxlen)\n",
    "print('embeded caption example')\n",
    "print(random.choice(list(encoded_train_captions.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return loss, optimizer, pred\n",
    "# vocab_size: 사용하는 총 단어의 수\n",
    "# maxlen: 문장에 등장하는 단어의 최대 횟수\n",
    "# hidden_size: lstm cell의 hidden state의 크기이자 output vector의 크기\n",
    "def build_model(vocab_size, hidden_size, learning_rate, embedding_size):\n",
    "    x_img = tf.placeholder(tf.float32, [None, 4096]) # image feature size = 4096\n",
    "    x_seq = tf.placeholder(tf.int32, [None, None]) # \n",
    "    y_seq = tf.placeholder(tf.int32, [None, None]) # \n",
    "    \n",
    "    img_embedding = tf.layers.dense(x_img, embedding_size, activation=tf.nn.relu) # 4096 to embedding_size\n",
    "    w_word_embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0)) #  vocab_size to embedding_size\n",
    "    word_embedding = tf.nn.embedding_lookup(w_word_embedding, x_seq)\n",
    "#     print('img_embedding:', img_embedding.shape)\n",
    "#     print('word_embedding:', word_embedding.shape)\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "    init_state = lstm.zero_state(batch_size=tf.shape(x_seq)[0], dtype=tf.float32)\n",
    "#     print('init_state:', init_state)\n",
    "    _, state = lstm(img_embedding, init_state)\n",
    "#     print('vocab_size:', vocab_size)\n",
    "#     print('x_seq.shape:', x_seq.shape)\n",
    "    outputs, state = tf.nn.dynamic_rnn(lstm, word_embedding, initial_state=state)\n",
    "    \n",
    "    logits = tf.layers.dense(outputs, vocab_size, activation=tf.nn.relu)\n",
    "    targets = tf.one_hot(y_seq, vocab_size)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=targets))\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    pred = tf.argmax(logits, axis=0)\n",
    "\n",
    "    \n",
    "    return x_img, x_seq, y_seq, loss, optimizer, pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoded_train_captions, train_features, embedding_size, epochs):\n",
    "    image_ids = train_features.keys()\n",
    "    tf.reset_default_graph()\n",
    "    x_img, x_seq, y_seq, loss, optimizer, pred = build_model(vocab_size, hidden_size, \n",
    "                                                         learning_rate, embedding_size)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        for i in range(epochs):\n",
    "            print('epoch_{}'.format(i+1))\n",
    "            start = time.time()\n",
    "            for i, image_id in enumerate(image_ids):\n",
    "                img_feature = train_features[image_id]\n",
    "                img_feature = np.expand_dims(img_feature, axis=0)\n",
    "                captions = encoded_train_captions[image_id]\n",
    "#                 print('img_feature.shape:', img_feature.shape)\n",
    "                for c in captions:\n",
    "                    c = np.array(c)\n",
    "                    c = np.expand_dims(c, axis=0)\n",
    "                    _x_seq = c[:, :-1]\n",
    "                    _y_seq = c[:, 1:]\n",
    "#                     print('_y_seq:', _y_seq.shape)\n",
    "                    feed = {\n",
    "                        x_img: img_feature,\n",
    "                        x_seq: c[:, :-1],\n",
    "                        y_seq: c[:, 1:]\n",
    "                    }\n",
    "                    l, _ = sess.run([loss, optimizer], feed)\n",
    "                    \n",
    "                print('iter: {}/{}, loss:{}'.format(i+1, len(image_ids), l), end='\\r')\n",
    "                \n",
    "            end = time.time()\n",
    "            print('Elapsed for this epoch:', end-start, 'sec')\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_1\n",
      "iter: 2051/6000, loss:5.5774159431457525\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-370-bc3b92febecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membedding_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_train_captions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-367-95fae75ba48f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoded_train_captions, train_features, embedding_size, epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m                         \u001b[0my_seq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     }\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iter: {}/{}, loss:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "hidden_size = 256\n",
    "epochs = 10\n",
    "embedding_size = 128\n",
    "\n",
    "train(encoded_train_captions, train_features, embedding_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
