{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nextract features for all images in out dataset(include valid set, test set)\\nby doing this, you can reduce for image feature extraction\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "extract features for all images in out dataset(include valid set, test set)\n",
    "by doing this, you can reduce for image feature extraction\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "import numpy as np\n",
    "import time\n",
    "import gzip\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 2.6946074962615967 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = VGG16()\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output) # drop last prediction layer\n",
    "end = time.time()\n",
    "print('Elapsed:', end-start, 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8090/8091-->997722733_0cb54394726\r"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "image_dir = os.getcwd() + '/Flicker8k_Dataset'\n",
    "features = dict()\n",
    "for i, filename in enumerate(os.listdir(image_dir)):\n",
    "    filepath = image_dir + '/' + filename\n",
    "    image = load_img(filepath, target_size=(224, 224, 3)) # VGG16 input size\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0) # according to keras document\n",
    "    image = preprocess_input(image) # according to keras document\n",
    "    feature = model.predict(image)\n",
    "    image_id = filename.split('.')[0]\n",
    "    features[image_id] = feature\n",
    "    print('{0:>4}/{1:<4}-->{2}'.format(i, len(os.listdir(image_dir)), image_id), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of images feature extracted: 8091\n",
      "Elapsed time: 1673.702288866043sec\n"
     ]
    }
   ],
   "source": [
    "print('# of images feature extracted:', len(features))\n",
    "end = time.time()\n",
    "print('Elapsed time: {}sec'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len total imagestotal_dataset_dataset: 8091\n",
      "len train valid test images: 8000\n"
     ]
    }
   ],
   "source": [
    "total_dataset = set([key for key in features])\n",
    "train_valid_test_set = set()\n",
    "train_info_filename = os.getcwd() + '/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "valid_info_filename = os.getcwd() + '/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "test_info_filename = os.getcwd() + '/Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "\n",
    "with open(train_info_filename, 'r') as f:\n",
    "    text = f.read()\n",
    "    image_filenames = text.split('\\n') # get image filenames in dataset: (train, valid, test)\n",
    "    image_ids = [x.split('.')[0] for x in image_filenames if x] # remove '.jpg', igore empty filename\n",
    "    train_valid_test_set.update(image_ids)\n",
    "\n",
    "with open(valid_info_filename, 'r') as f:\n",
    "    text = f.read()\n",
    "    image_filenames = text.split('\\n') # get image filenames in dataset: (train, valid, test)\n",
    "    image_ids = [x.split('.')[0] for x in image_filenames if x] # remove '.jpg', igore empty filename\n",
    "    train_valid_test_set.update(image_ids)\n",
    "                                \n",
    "with open(test_info_filename, 'r') as f:\n",
    "    text = f.read()\n",
    "    image_filenames = text.split('\\n') # get image filenames in dataset: (train, valid, test)\n",
    "    image_ids = [x.split('.')[0] for x in image_filenames if x] # remove '.jpg', igore empty filename\n",
    "    train_valid_test_set.update(image_ids)\n",
    "                                \n",
    "# There are unused images\n",
    "print('len total imagestotal_dataset_dataset:', len(total_dataset))\n",
    "print('len train valid test images:', len(train_valid_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 70.21755266189575 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with gzip.open('image_features.pkl.zip', 'wb') as f:\n",
    "    pickle.dump(features, f)\n",
    "    \n",
    "end = time.time()\n",
    "print('Elapsed:', end-start, 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
