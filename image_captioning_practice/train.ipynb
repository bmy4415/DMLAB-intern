{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# overal 8092 images, trainset in 'Flickr_8k.trainImages.txt'\n",
    "# overal 8092 images, validset in 'Flickr_8k.devImages.txt'\n",
    "# overal 8092 images, testset in 'Flickr_8k.testImages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(filename):\n",
    "    image_ids = list()\n",
    "    image_features = dict()\n",
    "    captions = dict()\n",
    "    \n",
    "    # get image id\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "        image_filenames = text.split('\\n') # get image filenames in dataset: (train, valid, test)\n",
    "        image_ids = [x.split('.')[0] for x in image_filenames if x] # remove '.jpg', igore empty filename\n",
    "\n",
    "    # get image feature\n",
    "    with open(os.getcwd() + '/image_features.pkl', 'rb') as f:\n",
    "        all_features = pickle.load(f)\n",
    "        image_features = { x: all_features[x] for x in image_ids }\n",
    "        \n",
    "    # get image caption\n",
    "    with open(os.getcwd() + '/captions.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "        lines = text.split('\\n')\n",
    "        for line in lines:\n",
    "            space = line.index(' ')\n",
    "            image_id = line[:space]\n",
    "            caption = line[space+1:]\n",
    "            \n",
    "            # ignore empty caption\n",
    "            if not caption:\n",
    "                continue\n",
    "\n",
    "            caption = 'startseq ' + caption + ' endseq' # add <start>, <end> token\n",
    "            \n",
    "            # append captions only in specified image_ids\n",
    "            if image_id not in image_ids:\n",
    "                continue\n",
    "\n",
    "            if image_id in captions:\n",
    "                captions[image_id].append(caption)\n",
    "            else:\n",
    "                captions[image_id] = [caption]\n",
    "                    \n",
    "    return image_ids, image_features, captions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# # get image id\n",
    "# with open(os.getcwd() + '/Flickr8k_text/Flickr_8k.trainImages.txt', 'r') as f:\n",
    "#     text = f.read()\n",
    "#     filenames = text.split('\\n')\n",
    "#     train_image_ids = [x.split('.')[0] for x in filenames if x] # remove '.jpg', ignore empty line\n",
    "\n",
    "# # get image feature\n",
    "# with open(os.getcwd() + '/image_features.pkl', 'rb') as f:\n",
    "#     all_features = pickle.load(f)\n",
    "#     train_features = { x: all_features[x] for x in train_image_ids }\n",
    "    \n",
    "# # get image caption\n",
    "# with open(os.getcwd() + '/captions.txt', 'r') as f:\n",
    "#     text = f.read()\n",
    "#     for line in text.split('\\n'):\n",
    "#         space_idx = line.index(' ')\n",
    "#         image_id = line[:space_idx]\n",
    "#         image_caption = line[space_idx+1:]\n",
    "#         image_caption = ['startseq'] + image_caption.split() + ['endseq'] # add <start>, <end> token, keras tokenizer will remove '<'(punctuation)\n",
    "#         image_caption = ' '.join(image_caption) # back to string\n",
    "#         if image_id in train_image_ids:\n",
    "#             if image_id in train_captions:\n",
    "#                 train_captions[image_id].append(image_caption)\n",
    "#             else:\n",
    "#                 train_captions[image_id] = [image_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of images in train set: 6000\n",
      "# of features in train set: 6000\n",
      "# of images in train caption dict: 6000\n",
      "# of images in valid set: 1000\n",
      "# of features in valid set: 1000\n",
      "# of images in valid caption dict: 1000\n",
      "-------------------------------------------------------------------\n",
      "Exmaples in each variable\n",
      "train_image_ids[0]: 2513260012_03d33305cf\n",
      "train_feature_dict[train_image_ids[0]].shape: (1, 4096)\n",
      "train_caption_dict[train_image_ids[0]]:\n",
      "['startseq black dog is running after white dog in the snow endseq',\n",
      " 'startseq black dog chasing brown dog through snow endseq',\n",
      " 'startseq two dogs chase each other across the snowy ground endseq',\n",
      " 'startseq two dogs play together in the snow endseq',\n",
      " 'startseq two dogs running through low lying body of water endseq']\n"
     ]
    }
   ],
   "source": [
    "# load each dataset\n",
    "\n",
    "train_info_filename = os.getcwd() + '/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "valid_info_filename = os.getcwd() + '/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "train_image_ids, train_feature_dict, train_caption_dict = prepare_dataset(train_info_filename)\n",
    "valid_image_ids, valid_feature_dict, valid_caption_dict = prepare_dataset(valid_info_filename)\n",
    "\n",
    "print('# of images in train set:', len(train_image_ids))\n",
    "print('# of features in train set:', len(train_feature_dict)) # this should be same with above\n",
    "print('# of images in train caption dict:', len(train_caption_dict))\n",
    "print('# of images in valid set:', len(valid_image_ids))\n",
    "print('# of features in valid set:', len(valid_feature_dict)) # this should be same with above\n",
    "print('# of images in valid caption dict:', len(valid_caption_dict))\n",
    "\n",
    "print('-------------------------------------------------------------------')\n",
    "print('Exmaples in each variable')\n",
    "print('train_image_ids[0]:', train_image_ids[0])\n",
    "print('train_feature_dict[train_image_ids[0]].shape:', train_feature_dict[train_image_ids[0]].shape)\n",
    "print('train_caption_dict[train_image_ids[0]]:')\n",
    "pprint(train_caption_dict[train_image_ids[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_captions(captions_dict):\n",
    "    result = []\n",
    "    for image_id in captions_dict:\n",
    "        caption_list = captions_dict[image_id]\n",
    "        [result.append(x) for x in caption_list]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(all_captions):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_captions)\n",
    "    vocab_size = len(tokenizer.word_index) + 1 # cf) https://github.com/keras-team/keras/issues/7551\n",
    "    return tokenizer, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len all train captions: 30000\n",
      "# of distinct word in all captions: 7578\n",
      "vocab_size: 7579\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tokenizer, vocab_size = get_tokenizer(get_all_captions(train_caption_dict))\n",
    "\n",
    "print('len all train captions:', len(get_all_captions(train_caption_dict)))\n",
    "\n",
    "word_set = set()\n",
    "for caption in get_all_captions(train_caption_dict):\n",
    "    caption = caption.split()[:]\n",
    "    word_set.update(caption)\n",
    "    \n",
    "print('# of distinct word in all captions:', len(word_set))\n",
    "print('vocab_size:', vocab_size)\n",
    "print(set(tokenizer.word_docs.keys()) == word_set) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get maximun length of caption\n",
    "def get_max_length(all_captions):\n",
    "    length = 0\n",
    "    for caption in all_captions:\n",
    "        if len(caption.split()) > length:\n",
    "            length = len(caption.split())\n",
    "            \n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "our model receive 2 inputs: image feature vector, sequence of words\n",
    "our model outputs 1 word\n",
    "if given image feature and caption: 'there is dog', we reuse subsequence of caption\n",
    "image, 'startseq' => 'there'\n",
    "image, 'startseq', 'there' => 'is'\n",
    "image, 'startseq', 'there', 'is' => 'dog'\n",
    "image, 'startseq', 'there', 'is', 'dog' => 'endseq'\n",
    "\n",
    "tokenizer: tokenizer from train set\n",
    "maxlen: maximum length of captions in train set\n",
    "vocab_size: vocab_size extracted from train set\n",
    "image_ids: image ids of data set which will be fed into model(train, valid, test)\n",
    "feature_dict: feature dict of data set which will be fed into model(train, valid, test)\n",
    "caption_dict: caption dict of data set which will be fed into model(train, valid, test)\n",
    "'''\n",
    "\n",
    "def get_subsequence_inputs(tokenizer, maxlen, vocab_size, image_ids, feature_dict, caption_dict):\n",
    "    image, subsequence, output = list(), list(), list()\n",
    "    for image_id in image_ids:\n",
    "        captions = caption_dict[image_id]\n",
    "        for caption in captions:\n",
    "            # convert 'word sequnce' to 'int sequence'\n",
    "            # each int implies word in vocab\n",
    "            vector = tokenizer.texts_to_sequences([caption])[0] \n",
    "            for i in range(1, len(vector)):\n",
    "                subseq, out = vector[:i], vector[i]\n",
    "                # add padding to make input sequence has same fixed length\n",
    "                subseq = pad_sequences([subseq], maxlen=maxlen)[0]\n",
    "                # one hot encoding\n",
    "                out = to_categorical([out], num_classes=vocab_size)[0]\n",
    "                image.append(feature_dict[image_id][0]) # image feature vector\n",
    "                subsequence.append(subseq) # vetorized subsequence of fixed size\n",
    "                output.append(out) # one-hot encoded word vector\n",
    "                \n",
    "    return np.array(image), np.array(subsequence), np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for image_id, caption_list in train_captions.items():\n",
    "#     for caption in caption_list:\n",
    "#         vec = tokenizer.texts_to_sequences([caption])[0] # convert to vector such that mapping each word to int based on vocab_size\n",
    "#         for i in range(1, len(vec)):\n",
    "#             seq_in, seq_out = vec[:i], vec[i] # split into pair\n",
    "#             seq_in = pad_sequences([seq_in], maxlen=max_caption_length)[0] # add padding to make all caption vector has same length\n",
    "#             seq_out = to_categorical([seq_out], num_classes=vocab_size)[0] # one hot encoding\n",
    "#             x1.append(train_features[image_id][0]) # feature vector\n",
    "#             x2.append(seq_in) # input word sequence to fixed length int vector: max length of input caption\n",
    "#             y.append(seq_out) # output word to one-hot encoded vector of fixed length: vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "maxlen = get_max_length(get_all_captions(train_caption_dict))\n",
    "x1_train, x2_train, y_train = get_subsequence_inputs(tokenizer, maxlen, vocab_size,\n",
    "                                                     train_image_ids, train_feature_dict, train_caption_dict)            \n",
    "x1_valid, x2_valid, y_valid = get_subsequence_inputs(tokenizer, maxlen, vocab_size,\n",
    "                                                     valid_image_ids, valid_feature_dict, valid_caption_dict)            \n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 57.90935921669006 sec\n",
      "maxlen: 34\n",
      "total # of inputs to model: 306404\n",
      "Example of subsequnce inputs below\n",
      "x1_train: (4096,)\n",
      "array([2.5076475, 0.       , 0.       , ..., 0.       , 0.       ,\n",
      "       0.       ], dtype=float32)\n",
      "x2_train: (34,)\n",
      "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 14,  8],\n",
      "      dtype=int32)\n",
      "y_train: (7579,)\n",
      "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('Elapsed time:', end-start, 'sec')\n",
    "print('maxlen:', maxlen)\n",
    "print('total # of inputs to model:', len(x1_train))\n",
    "print('Example of subsequnce inputs below')\n",
    "print('x1_train:', x1_train[2].shape) # image feature vector\n",
    "pprint(x1_train[2])\n",
    "print('x2_train:', x2_train[2].shape) # vectorized fixed length word sequence\n",
    "pprint(x2_train[2])\n",
    "print('y_train:',y_train[2].shape) # one hot encoded word vector\n",
    "pprint(y_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "# vocab_size, maxlen already described above\n",
    "def get_model(vocab_size, maxlen):\n",
    "    # image\n",
    "    inputs1 = Input(shape=(4096,)) # feature extracted from pretrained VGG16\n",
    "    image = Dropout(0.5)(inputs1) # dropout layer\n",
    "    image = Dense(256, activation='relu')(image) # fc layer outputs 256 features\n",
    "    \n",
    "    # caption\n",
    "    inputs2 = Input(shape=(maxlen,)) # vectorized word sequence\n",
    "    # word embedding base on 'vocab_size' to make simliar word has similar vector\n",
    "    # shape: (maxlen) -> (maxlen, 256)\n",
    "    # 'make_zero=True' option will ignore padded word\n",
    "    caption = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    caption = Dropout(0.5)(caption) # dropout layer\n",
    "    caption = LSTM(256)(caption) # LSTM layer\n",
    "    \n",
    "    # decoder\n",
    "    decoder = add([image, caption])\n",
    "    decoder = Dense(256, activation='relu')(decoder) # fc layer\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder) # probability of each vocab\n",
    "    \n",
    "    # make model\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam') # set loss function and optimizer\n",
    "    \n",
    "    # visualize model summary\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# model checkpoint\\nstart = time.time()\\n\\nfilepath = 'model-epoch{epoch:03d}-loss{loss:.3f}-validset_loss{val_loss:.3f}.h5' # format string\\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\\n\\n# fit model\\nmodel = get_model(vocab_size, maxlen)\\nprint('x1_train:', x1_train[2].shape) # image feature vector\\nprint('x2_train:', x2_train[2].shape) # vectorized fixed length word sequence\\nprint('y_train:',y_train[2].shape) # one hot encoded word vector\\n\\nprint('x1_valid:', x1_valid[2].shape) # image feature vector\\nprint('x2_valid:', x2_valid[2].shape) # vectorized fixed length word sequence\\nprint('y_valid:',y_valid[2].shape) # one hot encoded word vector\\n\\n\\nmodel.fit([x1_train, x2_train], y_train, epochs=1, verbose=2, callbacks=[checkpoint], validation_data=([x1_valid, x2_valid], y_valid))\\n\\nend = time.time()\\nprint('Elapsed:', end-start, 'sec')\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# possible to use this cell when you have enough memory\n",
    "'''\n",
    "# model checkpoint\n",
    "start = time.time()\n",
    "\n",
    "filepath = 'model-epoch{epoch:03d}-loss{loss:.3f}-validset_loss{val_loss:.3f}.h5' # format string\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# fit model\n",
    "model = get_model(vocab_size, maxlen)\n",
    "print('x1_train:', x1_train[2].shape) # image feature vector\n",
    "print('x2_train:', x2_train[2].shape) # vectorized fixed length word sequence\n",
    "print('y_train:',y_train[2].shape) # one hot encoded word vector\n",
    "\n",
    "print('x1_valid:', x1_valid[2].shape) # image feature vector\n",
    "print('x2_valid:', x2_valid[2].shape) # vectorized fixed length word sequence\n",
    "print('y_valid:',y_valid[2].shape) # one hot encoded word vector\n",
    "\n",
    "\n",
    "model.fit([x1_train, x2_train], y_train, epochs=1, verbose=2, callbacks=[checkpoint], validation_data=([x1_valid, x2_valid], y_valid))\n",
    "\n",
    "end = time.time()\n",
    "print('Elapsed:', end-start, 'sec')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator to fit in workstation memory\n",
    "def data_generator(caption_dict, feature_dict, tokenizer, maxlen):\n",
    "    while True:\n",
    "        for image_id, caption_list in caption_dict.items():\n",
    "            image_in = feature_dict[image_id][0]\n",
    "            image_in, seq_in, out_word = get_subsequence_inputs(tokenizer, maxlen, caption_list, image_in)\n",
    "            yield [[image_in, seq_in], out_word]\n",
    "            \n",
    "def get_subsequence_inputs(tokenizer, maxlen, caption_list, image_in):\n",
    "    x1, x2, y = list(), list(), list()\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    for caption in caption_list:\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "        \n",
    "        # make subsequence inputs\n",
    "        for i in range(1, len(seq)):\n",
    "            seq_in, out_word = seq[:i], seq[i]\n",
    "            seq_in = pad_sequences([seq_in], maxlen=maxlen)[0]\n",
    "            out_word = to_categorical([out_word], num_classes=vocab_size)[0]\n",
    "            x1.append(image_in)\n",
    "            x2.append(seq_in)\n",
    "            y.append(out_word)\n",
    "            \n",
    "    return np.array(x1), np.array(x2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 4096)\n",
      "(45, 34)\n",
      "(45, 7579)\n"
     ]
    }
   ],
   "source": [
    "generator = data_generator(train_caption_dict, train_feature_dict, tokenizer, maxlen)\n",
    "inputs, outputs = next(generator)\n",
    "print(inputs[0].shape)\n",
    "print(inputs[1].shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 34, 256)      1940224     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4096)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 34, 256)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7579)         1947803     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmy4415/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 855/6000 [===>..........................] - ETA: 38:20 - loss: 5.5303"
     ]
    }
   ],
   "source": [
    "# train using generator for saving memory\n",
    "model = get_model(vocab_size, maxlen)\n",
    "epochs = 1\n",
    "steps = len(train_caption_dict)\n",
    "start = time.time()\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_caption_dict, train_feature_dict, tokenizer, maxlen)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('model_{}.h5'.format(i))\n",
    "    \n",
    "end = time.time()\n",
    "print('Elaspsed:', end-start, 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
