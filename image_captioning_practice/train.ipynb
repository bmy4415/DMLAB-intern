{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# overal 8092 images, trainset in 'Flickr_8k.trainImages.txt'\n",
    "# overal 8092 images, validset in 'Flickr_8k.devImages.txt'\n",
    "# overal 8092 images, testset in 'Flickr_8k.testImages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(filename):\n",
    "    image_ids = list()\n",
    "    image_features = dict()\n",
    "    captions = dict()\n",
    "    \n",
    "    # get image id\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "        image_filenames = text.split('\\n') # get image filenames in dataset: (train, valid, test)\n",
    "        image_ids = [x.split('.')[0] for x in image_filenames if x] # remove '.jpg', igore empty filename\n",
    "\n",
    "    # get image feature\n",
    "    with open(os.getcwd() + '/image_features.pkl', 'rb') as f:\n",
    "        all_features = pickle.load(f)\n",
    "        image_features = { x: all_features[x] for x in image_ids }\n",
    "        \n",
    "    # get image caption\n",
    "    with open(os.getcwd() + '/captions.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "        lines = text.split('\\n')\n",
    "        for line in lines:\n",
    "            space = line.index(' ')\n",
    "            image_id = line[:space]\n",
    "            caption = line[space+1:]\n",
    "            \n",
    "            # ignore empty caption\n",
    "            if not caption:\n",
    "                continue\n",
    "\n",
    "            caption = 'startseq ' + caption + ' endseq' # add <start>, <end> token\n",
    "            \n",
    "            # append captions only in specified image_ids\n",
    "            if image_id not in image_ids:\n",
    "                continue\n",
    "\n",
    "            if image_id in captions:\n",
    "                captions[image_id].append(caption)\n",
    "            else:\n",
    "                captions[image_id] = [caption]\n",
    "                    \n",
    "    return image_ids, image_features, captions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# # get image id\n",
    "# with open(os.getcwd() + '/Flickr8k_text/Flickr_8k.trainImages.txt', 'r') as f:\n",
    "#     text = f.read()\n",
    "#     filenames = text.split('\\n')\n",
    "#     train_image_ids = [x.split('.')[0] for x in filenames if x] # remove '.jpg', ignore empty line\n",
    "\n",
    "# # get image feature\n",
    "# with open(os.getcwd() + '/image_features.pkl', 'rb') as f:\n",
    "#     all_features = pickle.load(f)\n",
    "#     train_features = { x: all_features[x] for x in train_image_ids }\n",
    "    \n",
    "# # get image caption\n",
    "# with open(os.getcwd() + '/captions.txt', 'r') as f:\n",
    "#     text = f.read()\n",
    "#     for line in text.split('\\n'):\n",
    "#         space_idx = line.index(' ')\n",
    "#         image_id = line[:space_idx]\n",
    "#         image_caption = line[space_idx+1:]\n",
    "#         image_caption = ['startseq'] + image_caption.split() + ['endseq'] # add <start>, <end> token, keras tokenizer will remove '<'(punctuation)\n",
    "#         image_caption = ' '.join(image_caption) # back to string\n",
    "#         if image_id in train_image_ids:\n",
    "#             if image_id in train_captions:\n",
    "#                 train_captions[image_id].append(image_caption)\n",
    "#             else:\n",
    "#                 train_captions[image_id] = [image_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of images in train set: 6000\n",
      "# of features in train set: 6000\n",
      "# of images in train caption dict: 6000\n",
      "# of images in valid set: 1000\n",
      "# of features in valid set: 1000\n",
      "# of images in valid caption dict: 1000\n",
      "-------------------------------------------------------------------\n",
      "Exmaples in each variable\n",
      "train_image_ids[0]: 2513260012_03d33305cf\n",
      "train_feature_dict[train_image_ids[0]].shape: (1, 4096)\n",
      "train_caption_dict[train_image_ids[0]]:\n",
      "['startseq black dog is running after white dog in the snow endseq',\n",
      " 'startseq black dog chasing brown dog through snow endseq',\n",
      " 'startseq two dogs chase each other across the snowy ground endseq',\n",
      " 'startseq two dogs play together in the snow endseq',\n",
      " 'startseq two dogs running through low lying body of water endseq']\n"
     ]
    }
   ],
   "source": [
    "# load each dataset\n",
    "\n",
    "train_info_filename = os.getcwd() + '/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "valid_info_filename = os.getcwd() + '/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "train_image_ids, train_feature_dict, train_caption_dict = prepare_dataset(train_info_filename)\n",
    "valid_image_ids, valid_feature_dict, valid_caption_dict = prepare_dataset(valid_info_filename)\n",
    "\n",
    "print('# of images in train set:', len(train_image_ids))\n",
    "print('# of features in train set:', len(train_feature_dict)) # this should be same with above\n",
    "print('# of images in train caption dict:', len(train_caption_dict))\n",
    "print('# of images in valid set:', len(valid_image_ids))\n",
    "print('# of features in valid set:', len(valid_feature_dict)) # this should be same with above\n",
    "print('# of images in valid caption dict:', len(valid_caption_dict))\n",
    "\n",
    "print('-------------------------------------------------------------------')\n",
    "print('Exmaples in each variable')\n",
    "print('train_image_ids[0]:', train_image_ids[0])\n",
    "print('train_feature_dict[train_image_ids[0]].shape:', train_feature_dict[train_image_ids[0]].shape)\n",
    "print('train_caption_dict[train_image_ids[0]]:')\n",
    "pprint(train_caption_dict[train_image_ids[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_captions(captions_dict):\n",
    "    result = []\n",
    "    for image_id in captions_dict:\n",
    "        caption_list = captions_dict[image_id]\n",
    "        [result.append(x) for x in caption_list]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(all_captions):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_captions)\n",
    "    vocab_size = len(tokenizer.word_index) + 1 # cf) https://github.com/keras-team/keras/issues/7551\n",
    "    return tokenizer, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len all train captions: 30000\n",
      "# of distinct word in all captions: 7578\n",
      "vocab_size: 7579\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tokenizer, vocab_size = get_tokenizer(get_all_captions(train_caption_dict))\n",
    "\n",
    "print('len all train captions:', len(get_all_captions(train_caption_dict)))\n",
    "\n",
    "word_set = set()\n",
    "for caption in get_all_captions(train_caption_dict):\n",
    "    caption = caption.split()[:]\n",
    "    word_set.update(caption)\n",
    "    \n",
    "print('# of distinct word in all captions:', len(word_set))\n",
    "print('vocab_size:', vocab_size)\n",
    "print(set(tokenizer.word_docs.keys()) == word_set) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get maximun length of caption\n",
    "def get_max_length(all_captions):\n",
    "    length = 0\n",
    "    for caption in all_captions:\n",
    "        if len(caption.split()) > length:\n",
    "            length = len(caption.split())\n",
    "            \n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "our model receive 2 inputs: image feature vector, sequence of words\n",
    "our model outputs 1 word\n",
    "if given image feature and caption: 'there is dog', we reuse subsequence of caption\n",
    "image, 'startseq' => 'there'\n",
    "image, 'startseq', 'there' => 'is'\n",
    "image, 'startseq', 'there', 'is' => 'dog'\n",
    "image, 'startseq', 'there', 'is', 'dog' => 'endseq'\n",
    "\n",
    "tokenizer: tokenizer from train set\n",
    "maxlen: maximum length of captions in train set\n",
    "vocab_size: vocab_size extracted from train set\n",
    "image_ids: image ids of data set which will be fed into model(train, valid, test)\n",
    "feature_dict: feature dict of data set which will be fed into model(train, valid, test)\n",
    "caption_dict: caption dict of data set which will be fed into model(train, valid, test)\n",
    "'''\n",
    "\n",
    "def get_subsequence_inputs(tokenizer, maxlen, vocab_size, image_ids, feature_dict, caption_dict):\n",
    "    image, subsequence, output = list(), list(), list()\n",
    "    for image_id in image_ids:\n",
    "        captions = caption_dict[image_id]\n",
    "        for caption in captions:\n",
    "            # convert 'word sequnce' to 'int sequence'\n",
    "            # each int implies word in vocab\n",
    "            vector = tokenizer.texts_to_sequences([caption])[0] \n",
    "            for i in range(1, len(vector)):\n",
    "                subseq, out = vector[:i], vector[i]\n",
    "                # add padding to make input sequence has same fixed length\n",
    "                subseq = pad_sequences([subseq], maxlen=maxlen)[0]\n",
    "                # one hot encoding\n",
    "                out = to_categorical([out], num_classes=vocab_size)[0]\n",
    "                image.append(feature_dict[image_id][0]) # image feature vector\n",
    "                subsequence.append(subseq) # vetorized subsequence of fixed size\n",
    "                output.append(out) # one-hot encoded word vector\n",
    "                \n",
    "    return np.array(image), np.array(subsequence), np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for image_id, caption_list in train_captions.items():\n",
    "#     for caption in caption_list:\n",
    "#         vec = tokenizer.texts_to_sequences([caption])[0] # convert to vector such that mapping each word to int based on vocab_size\n",
    "#         for i in range(1, len(vec)):\n",
    "#             seq_in, seq_out = vec[:i], vec[i] # split into pair\n",
    "#             seq_in = pad_sequences([seq_in], maxlen=max_caption_length)[0] # add padding to make all caption vector has same length\n",
    "#             seq_out = to_categorical([seq_out], num_classes=vocab_size)[0] # one hot encoding\n",
    "#             x1.append(train_features[image_id][0]) # feature vector\n",
    "#             x2.append(seq_in) # input word sequence to fixed length int vector: max length of input caption\n",
    "#             y.append(seq_out) # output word to one-hot encoded vector of fixed length: vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "maxlen = get_max_length(get_all_captions(train_caption_dict))\n",
    "x1_train, x2_train, y_train = get_subsequence_inputs(tokenizer, maxlen, vocab_size,\n",
    "                                                     train_image_ids, train_feature_dict, train_caption_dict)            \n",
    "x1_valid, x2_valid, y_valid = get_subsequence_inputs(tokenizer, maxlen, vocab_size,\n",
    "                                                     valid_image_ids, valid_feature_dict, valid_caption_dict)            \n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 57.90935921669006 sec\n",
      "maxlen: 34\n",
      "total # of inputs to model: 306404\n",
      "Example of subsequnce inputs below\n",
      "x1_train: (4096,)\n",
      "array([2.5076475, 0.       , 0.       , ..., 0.       , 0.       ,\n",
      "       0.       ], dtype=float32)\n",
      "x2_train: (34,)\n",
      "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 14,  8],\n",
      "      dtype=int32)\n",
      "y_train: (7579,)\n",
      "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('Elapsed time:', end-start, 'sec')\n",
    "print('maxlen:', maxlen)\n",
    "print('total # of inputs to model:', len(x1_train))\n",
    "print('Example of subsequnce inputs below')\n",
    "print('x1_train:', x1_train[2].shape) # image feature vector\n",
    "pprint(x1_train[2])\n",
    "print('x2_train:', x2_train[2].shape) # vectorized fixed length word sequence\n",
    "pprint(x2_train[2])\n",
    "print('y_train:',y_train[2].shape) # one hot encoded word vector\n",
    "pprint(y_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "# vocab_size, maxlen already described above\n",
    "def get_model(vocab_size, maxlen):\n",
    "    # image\n",
    "    inputs1 = Input(shape=(4096,)) # feature extracted from pretrained VGG16\n",
    "    image = Dropout(0.5)(inputs1) # dropout layer\n",
    "    image = Dense(256, activation='relu')(image) # fc layer outputs 256 features\n",
    "    \n",
    "    # caption\n",
    "    inputs2 = Input(shape=(maxlen,)) # vectorized word sequence\n",
    "    # word embedding base on 'vocab_size' to make simliar word has similar vector\n",
    "    # shape: (maxlen) -> (maxlen, 256)\n",
    "    # 'make_zero=True' option will ignore padded word\n",
    "    caption = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    caption = Dropout(0.5)(caption) # dropout layer\n",
    "    caption = LSTM(256)(caption) # LSTM layer\n",
    "    \n",
    "    # decoder\n",
    "    decoder = add([image, caption])\n",
    "    decoder = Dense(256, activation='relu')(decoder) # fc layer\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder) # probability of each vocab\n",
    "    \n",
    "    # make model\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam') # set loss function and optimizer\n",
    "    \n",
    "    # visualize model summary\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# model checkpoint\\nstart = time.time()\\n\\nfilepath = 'model-epoch{epoch:03d}-loss{loss:.3f}-validset_loss{val_loss:.3f}.h5' # format string\\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\\n\\n# fit model\\nmodel = get_model(vocab_size, maxlen)\\nprint('x1_train:', x1_train[2].shape) # image feature vector\\nprint('x2_train:', x2_train[2].shape) # vectorized fixed length word sequence\\nprint('y_train:',y_train[2].shape) # one hot encoded word vector\\n\\nprint('x1_valid:', x1_valid[2].shape) # image feature vector\\nprint('x2_valid:', x2_valid[2].shape) # vectorized fixed length word sequence\\nprint('y_valid:',y_valid[2].shape) # one hot encoded word vector\\n\\n\\nmodel.fit([x1_train, x2_train], y_train, epochs=1, verbose=2, callbacks=[checkpoint], validation_data=([x1_valid, x2_valid], y_valid))\\n\\nend = time.time()\\nprint('Elapsed:', end-start, 'sec')\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# possible to use this cell when you have enough memory\n",
    "'''\n",
    "# model checkpoint\n",
    "start = time.time()\n",
    "\n",
    "filepath = 'model-epoch{epoch:03d}-loss{loss:.3f}-validset_loss{val_loss:.3f}.h5' # format string\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# fit model\n",
    "model = get_model(vocab_size, maxlen)\n",
    "print('x1_train:', x1_train[2].shape) # image feature vector\n",
    "print('x2_train:', x2_train[2].shape) # vectorized fixed length word sequence\n",
    "print('y_train:',y_train[2].shape) # one hot encoded word vector\n",
    "\n",
    "print('x1_valid:', x1_valid[2].shape) # image feature vector\n",
    "print('x2_valid:', x2_valid[2].shape) # vectorized fixed length word sequence\n",
    "print('y_valid:',y_valid[2].shape) # one hot encoded word vector\n",
    "\n",
    "\n",
    "model.fit([x1_train, x2_train], y_train, epochs=1, verbose=2, callbacks=[checkpoint], validation_data=([x1_valid, x2_valid], y_valid))\n",
    "\n",
    "end = time.time()\n",
    "print('Elapsed:', end-start, 'sec')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator to fit in workstation memory\n",
    "def data_generator(caption_dict, feature_dict, tokenizer, maxlen):\n",
    "    while True:\n",
    "        for image_id, caption_list in caption_dict.items():\n",
    "            image_in = feature_dict[image_id][0]\n",
    "            image_in, seq_in, out_word = get_subsequence_inputs(tokenizer, maxlen, caption_list, image_in)\n",
    "            yield [[image_in, seq_in], out_word]\n",
    "            \n",
    "def get_subsequence_inputs(tokenizer, maxlen, caption_list, image_in):\n",
    "    x1, x2, y = list(), list(), list()\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    for caption in caption_list:\n",
    "        print('caption:', caption)\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "        \n",
    "        # make subsequence inputs\n",
    "        for i in range(1, len(seq)):\n",
    "            seq_in, out_word = seq[:i], seq[i]\n",
    "            print('out_word:', out_word)\n",
    "            seq_in = pad_sequences([seq_in], maxlen=maxlen)[0]\n",
    "            out_word = to_categorical([out_word], num_classes=vocab_size)[0]\n",
    "            x1.append(image_in)\n",
    "            x2.append(seq_in)\n",
    "            y.append(out_word)\n",
    "            \n",
    "    return np.array(x1), np.array(x2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 4096)\n",
      "(45, 34)\n",
      "(45, 7579)\n"
     ]
    }
   ],
   "source": [
    "generator = data_generator(train_caption_dict, train_feature_dict, tokenizer, maxlen)\n",
    "inputs, outputs = next(generator)\n",
    "print(inputs[0].shape)\n",
    "print(inputs[1].shape)\n",
    "print(outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 34, 256)      1940224     input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 4096)         0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 34, 256)      0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 256)          1048832     dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 256)          525312      dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 256)          0           dense_15[0][0]                   \n",
      "                                                                 lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 256)          65792       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 7579)         1947803     dense_16[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "caption: startseq boy in red jacket is jumping in the sand endseq\n",
      "out_word: 15\n",
      "out_word: 3\n",
      "out_word: 25\n",
      "out_word: 94\n",
      "out_word: 6\n",
      "out_word: 44\n",
      "out_word: 3\n",
      "out_word: 4\n",
      "out_word: 130\n",
      "out_word: 2\n",
      "caption: startseq child is wearing red shirt and raising their arms endseq\n",
      "out_word: 42\n",
      "out_word: 6\n",
      "out_word: 20\n",
      "out_word: 25\n",
      "out_word: 36\n",
      "out_word: 7\n",
      "out_word: 1290\n",
      "out_word: 93\n",
      "out_word: 200\n",
      "out_word: 2\n",
      "caption: startseq child plays on the dirt endseq\n",
      "out_word: 42\n",
      "out_word: 110\n",
      "out_word: 5\n",
      "out_word: 4\n",
      "out_word: 108\n",
      "out_word: 2\n",
      "caption: startseq small child jumps high above sandy field endseq\n",
      "out_word: 51\n",
      "out_word: 42\n",
      "out_word: 68\n",
      "out_word: 194\n",
      "out_word: 232\n",
      "out_word: 361\n",
      "out_word: 49\n",
      "out_word: 2\n",
      "caption: startseq boy in red jumping forward on sand hill in background endseq\n",
      "out_word: 15\n",
      "out_word: 3\n",
      "out_word: 25\n",
      "out_word: 44\n",
      "out_word: 1109\n",
      "out_word: 5\n",
      "out_word: 130\n",
      "out_word: 129\n",
      "out_word: 3\n",
      "out_word: 105\n",
      "out_word: 2\n",
      "----------------------------\n",
      "caption: startseq boy is riding buggy down woodland path endseq\n",
      "out_word: 15\n",
      "out_word: 6\n",
      "out_word: 74\n",
      "out_word: 3472\n",
      "out_word: 35\n",
      "out_word: 1291\n",
      "out_word: 186\n",
      "out_word: 2\n",
      "caption: startseq person rolls down hill riding wagon as another watches endseq\n",
      "out_word: 43\n",
      "out_word: 1110\n",
      "out_word: 35\n",
      "out_word: 129\n",
      "out_word: 74\n",
      "out_word: 960\n",
      "out_word: 79\n",
      "out_word: 70\n",
      "out_word: 197\n",
      "out_word: 2\n",
      "caption: startseq young person riding small wagon down hill endseq\n",
      "out_word: 26\n",
      "out_word: 43\n",
      "out_word: 74\n",
      "out_word: 51\n",
      "out_word: 960\n",
      "out_word: 35\n",
      "out_word: 129\n",
      "out_word: 2\n",
      "caption: startseq the boy comes down the hill in the red wagon endseq\n",
      "out_word: 4\n",
      "out_word: 15\n",
      "out_word: 1352\n",
      "out_word: 35\n",
      "out_word: 4\n",
      "out_word: 129\n",
      "out_word: 3\n",
      "out_word: 4\n",
      "out_word: 25\n",
      "out_word: 960\n",
      "out_word: 2\n",
      "caption: startseq this person seated on type of gocart is riding downhill endseq\n",
      "out_word: 306\n",
      "out_word: 43\n",
      "out_word: 764\n",
      "out_word: 5\n",
      "out_word: 1187\n",
      "out_word: 11\n",
      "out_word: 2915\n",
      "out_word: 6\n",
      "out_word: 74\n",
      "out_word: 727\n",
      "out_word: 2\n",
      "----------------------------\n",
      "caption: startseq group of indian men are just standing around with their arms crossed endseq\n",
      "out_word: 54\n",
      "out_word: 11\n",
      "out_word: 1292\n",
      "out_word: 61\n",
      "out_word: 16\n",
      "out_word: 665\n",
      "out_word: 38\n",
      "out_word: 97\n",
      "out_word: 9\n",
      "out_word: 93\n",
      "out_word: 200\n",
      "out_word: 1293\n",
      "out_word: 2\n",
      "caption: startseq group of men stand outside endseq\n",
      "out_word: 54\n",
      "out_word: 11\n",
      "out_word: 61\n",
      "out_word: 111\n",
      "out_word: 80\n",
      "out_word: 2\n",
      "caption: startseq four men from another country look at the camera endseq\n",
      "out_word: 122\n",
      "out_word: 61\n",
      "out_word: 73\n",
      "out_word: 70\n",
      "out_word: 1353\n",
      "out_word: 212\n",
      "out_word: 21\n",
      "out_word: 4\n",
      "out_word: 91\n",
      "out_word: 2\n",
      "caption: startseq men wearing hats stand with arms folded endseq\n",
      "out_word: 61\n",
      "out_word: 20\n",
      "out_word: 413\n",
      "out_word: 111\n",
      "out_word: 9\n",
      "out_word: 200\n",
      "out_word: 1890\n",
      "out_word: 2\n",
      "caption: startseq several men wearing ethnic hats are watching the photographer in an outdoor market endseq\n",
      "out_word: 178\n",
      "out_word: 61\n",
      "out_word: 20\n",
      "out_word: 3473\n",
      "out_word: 413\n",
      "out_word: 16\n",
      "out_word: 205\n",
      "out_word: 4\n",
      "out_word: 1009\n",
      "out_word: 3\n",
      "out_word: 27\n",
      "out_word: 372\n",
      "out_word: 812\n",
      "out_word: 2\n",
      "----------------------------\n",
      "caption: startseq little boy wearing an orange shirt and white shorts jumps barefoot on plastic jumper on top of the green grass endseq\n",
      "out_word: 39\n",
      "out_word: 15\n",
      "out_word: 20\n",
      "out_word: 27\n",
      "out_word: 85\n",
      "out_word: 36\n",
      "out_word: 7\n",
      "out_word: 13\n",
      "out_word: 159\n",
      "out_word: 68\n",
      "out_word: 709\n",
      "out_word: 5\n",
      "out_word: 330\n",
      "out_word: 1477\n",
      "out_word: 5\n",
      "out_word: 114\n",
      "out_word: 11\n",
      "out_word: 4\n",
      "out_word: 53\n",
      "out_word: 40\n",
      "out_word: 2\n",
      "caption: startseq young boy in red shirt plays on minitrampoline in grassy field endseq\n",
      "out_word: 26\n",
      "out_word: 15\n",
      "out_word: 3\n",
      "out_word: 25\n",
      "out_word: 36\n",
      "out_word: 110\n",
      "out_word: 5\n",
      "out_word: 4474\n",
      "out_word: 3\n",
      "out_word: 124\n",
      "out_word: 49\n",
      "out_word: 2\n",
      "caption: startseq boy wearing red shirt standing on plastic object and holding yellow toy shovel endseq\n",
      "out_word: 15\n",
      "out_word: 20\n",
      "out_word: 25\n",
      "out_word: 36\n",
      "out_word: 38\n",
      "out_word: 5\n",
      "out_word: 330\n",
      "out_word: 290\n",
      "out_word: 7\n",
      "out_word: 50\n",
      "out_word: 52\n",
      "out_word: 100\n",
      "out_word: 1406\n",
      "out_word: 2\n",
      "caption: startseq boy with yellow toy in hand jumps on toy on green grass endseq\n",
      "out_word: 15\n",
      "out_word: 9\n",
      "out_word: 52\n",
      "out_word: 100\n",
      "out_word: 3\n",
      "out_word: 171\n",
      "out_word: 68\n",
      "out_word: 5\n",
      "out_word: 100\n",
      "out_word: 5\n",
      "out_word: 53\n",
      "out_word: 40\n",
      "out_word: 2\n",
      "caption: startseq young boy bouncing off trampoline endseq\n",
      "out_word: 26\n",
      "out_word: 15\n",
      "out_word: 1111\n",
      "out_word: 90\n",
      "out_word: 393\n",
      "out_word: 2\n",
      "----------------------------\n",
      "caption: startseq group of children sit on skateboard half pipe endseq\n",
      "out_word: 54\n",
      "out_word: 11\n",
      "out_word: 62\n",
      "out_word: 160\n",
      "out_word: 5\n",
      "out_word: 132\n",
      "out_word: 987\n",
      "out_word: 900\n",
      "out_word: 2\n",
      "caption: startseq group of young children sit on the edge of cement ledge endseq\n",
      "out_word: 54\n",
      "out_word: 11\n",
      "out_word: 26\n",
      "out_word: 62\n",
      "out_word: 160\n",
      "out_word: 5\n",
      "out_word: 4\n",
      "out_word: 273\n",
      "out_word: 11\n",
      "out_word: 501\n",
      "out_word: 480\n",
      "out_word: 2\n",
      "caption: startseq children sit at the top of cement wall endseq\n",
      "out_word: 62\n",
      "out_word: 160\n",
      "out_word: 21\n",
      "out_word: 4\n",
      "out_word: 114\n",
      "out_word: 11\n",
      "out_word: 501\n",
      "out_word: 103\n",
      "out_word: 2\n",
      "caption: startseq six girls sit on painted pavement endseq\n",
      "out_word: 551\n",
      "out_word: 78\n",
      "out_word: 160\n",
      "out_word: 5\n",
      "out_word: 560\n",
      "out_word: 666\n",
      "out_word: 2\n",
      "caption: startseq six little girls are sitting on cement hill endseq\n",
      "out_word: 551\n",
      "out_word: 39\n",
      "out_word: 78\n",
      "out_word: 16\n",
      "out_word: 47\n",
      "out_word: 5\n",
      "out_word: 501\n",
      "out_word: 129\n",
      "out_word: 2\n",
      "----------------------------\n",
      "Elaspsed: 0.08302688598632812 sec\n"
     ]
    }
   ],
   "source": [
    "# train using generator for saving memory\n",
    "model = get_model(vocab_size, maxlen)\n",
    "epochs = 1\n",
    "steps = len(train_caption_dict)\n",
    "start = time.time()\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_caption_dict, train_feature_dict, tokenizer, maxlen)\n",
    "    for i in range(5):\n",
    "        inputs, outputs = next(generator)\n",
    "        print('----------------------------')\n",
    "#     model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "#     model.save('model_{}.h5'.format(i))\n",
    "    \n",
    "end = time.time()\n",
    "print('Elaspsed:', end-start, 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genetrating caption\n",
    "idx_to_word = dict()\n",
    "for word in tokenizer.word_index:\n",
    "    idx = tokenizer.word_index[word]\n",
    "    idx_to_word[idx] = word\n",
    "    \n",
    "    \n",
    "# given image feature and model, generate caption for image\n",
    "# 'startseq' and image feature will be fed into model\n",
    "# create each word until 'endseq' or reach maxlen\n",
    "def generate_caption(model, tokenizer, image_feature, maxlen):\n",
    "    seq_in = 'startseq'\n",
    "    \n",
    "    for i in range(maxlen):\n",
    "        seq_vector = tokenizer.texts_to_sequences([seq_in])[0]\n",
    "        seq_vector = pad_sequences([seq_vector], maxlen=maxlen) # add padding to sequence vector\n",
    "        pred_word = model.predict([image_feature, seq_vector], verbose=2) # predict next word\n",
    "        pred_word = np.argmax(pred_word) # most probable word's index\n",
    "        word = idx_to_word.get(pred_word, None)\n",
    "\n",
    "        # may be error case\n",
    "        if word is None:\n",
    "            print('index not in idx_to_word:', pred_word)\n",
    "            break\n",
    "            \n",
    "        if word == 'endseq':\n",
    "            seq_in += ' ' + word\n",
    "            break\n",
    "            \n",
    "        seq_in += ' ' + word\n",
    "\n",
    "    return seq_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy\n",
      "group\n",
      "little\n"
     ]
    }
   ],
   "source": [
    "print(idx_to_word[15])\n",
    "print(idx_to_word[54])\n",
    "print(idx_to_word[39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate performance\n",
    "def evaluate_model(model, y_caption_dict, y_feature_dict, tokenizer, maxlen):\n",
    "    y, pred = list(), list()\n",
    "    for image_id, caption_list in y_caption_dict.items():\n",
    "        _pred = generate_caption(model, tokenizer, y_feature_dict[image_id], maxlen) # generate caption\n",
    "        y.append([c.split() for c in caption_list]) # collect answer captions\n",
    "        pred.append(_pred.split()) # collect prediction\n",
    "        print(image_id, _pred)\n",
    "        \n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(y, pred, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(y, pred, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(y, pred, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(y, pred, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of images in test set: 1000\n",
      "# of features in test set: 1000\n"
     ]
    }
   ],
   "source": [
    "# prepare test set\n",
    "test_info_filename = os.getcwd() + '/Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "test_image_ids, test_feature_dict, test_caption_dict = prepare_dataset(test_info_filename)\n",
    "print('# of images in test set:', len(test_image_ids))\n",
    "print('# of features in test set:', len(test_feature_dict)) # this should be same with above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506738508_327efdf9c3 startseq man in red shirt is standing on the street endseq\n",
      "1982852140_56425fa7a2 startseq man in red shirt is standing on the street endseq\n",
      "3232470286_903a61ea16 startseq man in red shirt is standing on the street endseq\n",
      "2844641033_dab3715a99 startseq man in red shirt is standing on the street endseq\n",
      "3245070961_8977fdd548 startseq man in red shirt is standing on the street endseq\n",
      "293327462_20dee0de56 startseq man in red shirt is standing on the street endseq\n",
      "476759700_8911f087f8 startseq man in red shirt is standing on the street endseq\n",
      "3406930103_4db7b4dde0 startseq man in red shirt is standing on the street endseq\n",
      "2602258549_7401a3cdae startseq man in red shirt is standing on the street endseq\n",
      "3354330935_de75be9d2f startseq man in red shirt is standing on the street endseq\n",
      "1626754053_81126b67b6 startseq man in red shirt is standing on the street endseq\n",
      "670609997_5c7fdb3f0b startseq man in red shirt is standing on the street endseq\n",
      "3562050678_4196a7fff3 startseq man in red shirt is standing on the street endseq\n",
      "468608014_09fd20eb9b startseq man in red shirt is standing on the street endseq\n",
      "254295381_d98fa049f4 startseq man in red shirt is standing on the street endseq\n",
      "3490736665_38710f4b91 startseq man in red shirt is standing on the street endseq\n",
      "2248487950_c62d0c81a9 startseq man in red shirt is standing on the street endseq\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-dba481d6c60f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model_0.h5'\u001b[0m \u001b[0;31m# 1 epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_caption_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_feature_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-110-83c2dbd20dd3>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, y_caption_dict, y_feature_dict, tokenizer, maxlen)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_caption_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0m_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_feature_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# generate caption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# collect answer captions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# collect prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-6f04a88c619b>\u001b[0m in \u001b[0;36mgenerate_caption\u001b[0;34m(model, tokenizer, image_feature, maxlen)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mseq_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mseq_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add padding to sequence vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mpred_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# predict next word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mpred_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_word\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# most probable word's index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx_to_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m       return training_arrays.predict_loop(\n\u001b[0;32m-> 1878\u001b[0;31m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, inputs, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load model and evaluate\n",
    "start = time.time()\n",
    "\n",
    "model_filename = 'model_0.h5' # 1 epoch\n",
    "model1 = load_model(model_filename)\n",
    "evaluate_model(model1, test_caption_dict, test_feature_dict, tokenizer, maxlen)\n",
    "\n",
    "end = time.time()\n",
    "print('Elapsed:', end-start, 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506738508_327efdf9c3 startseq man in black shirt and hat is sitting on bench endseq\n",
      "1982852140_56425fa7a2 startseq man in black shirt and hat is sitting on bench endseq\n",
      "3232470286_903a61ea16 startseq man in black shirt and hat is sitting on bench endseq\n",
      "2844641033_dab3715a99 startseq man in black shirt and hat is sitting on bench endseq\n",
      "3245070961_8977fdd548 startseq man in black shirt and hat is sitting on bench endseq\n",
      "293327462_20dee0de56 startseq man in black shirt and hat is sitting on bench endseq\n",
      "476759700_8911f087f8 startseq man in black shirt and hat is sitting on bench endseq\n",
      "3406930103_4db7b4dde0 startseq man in black shirt and hat is sitting on bench endseq\n",
      "2602258549_7401a3cdae startseq man in black shirt and hat is sitting on bench endseq\n",
      "3354330935_de75be9d2f startseq man in black shirt and hat is sitting on bench endseq\n",
      "1626754053_81126b67b6 startseq man in black shirt and hat is sitting on bench endseq\n",
      "670609997_5c7fdb3f0b startseq man in black shirt and hat is sitting on bench endseq\n",
      "3562050678_4196a7fff3 startseq man in black shirt and hat is sitting on bench endseq\n",
      "468608014_09fd20eb9b startseq man in black shirt and hat is sitting on bench endseq\n",
      "254295381_d98fa049f4 startseq man in black shirt and hat is sitting on bench endseq\n",
      "3490736665_38710f4b91 startseq man in black shirt and hat is sitting on bench endseq\n",
      "2248487950_c62d0c81a9 startseq man in black shirt and hat is sitting on bench endseq\n",
      "2884301336_dc8e974431 startseq man in black shirt and hat is sitting on bench endseq\n",
      "2061354254_faa5bd294b startseq man in black shirt and hat is sitting on bench endseq\n",
      "2204550058_2707d92338 startseq man in black shirt and hat is sitting on bench endseq\n",
      "2358561039_e215a8d6cd startseq man in black shirt and hat is sitting on bench endseq\n",
      "2461616306_3ee7ac1b4b startseq man in black shirt and hat is sitting on bench endseq\n",
      "2588927489_f4da2f11ec startseq man in black shirt and hat is sitting on bench endseq\n",
      "3247052319_da8aba1983 startseq man in black shirt and hat is sitting on bench endseq\n",
      "189721896_1ffe76d89e startseq man in black shirt and hat is sitting on bench endseq\n",
      "2518508760_68d8df7365 startseq man in black shirt and hat is sitting on bench endseq\n",
      "3694064560_467683205b startseq man in black shirt and hat is sitting on bench endseq\n",
      "3471841031_a949645ba8 startseq man in black shirt and hat is sitting on bench endseq\n",
      "2534502836_7a75305655 startseq man in black shirt and hat is sitting on bench endseq\n",
      "3597326009_3678a98a43 startseq man in black shirt and hat is sitting on bench endseq\n",
      "2196316998_3b2d63f01f startseq man in black shirt and hat is sitting on bench endseq\n",
      "2339106348_2df90aa6a9 startseq man in black shirt and hat is sitting on bench endseq\n",
      "444057017_f1e0fcaef7 startseq man in black shirt and hat is sitting on bench endseq\n",
      "3502343542_f9b46688e5 startseq man in black shirt and hat is sitting on bench endseq\n",
      "3539767254_c598b8e6c7 startseq man in black shirt and hat is sitting on bench endseq\n",
      "1119015538_e8e796281e startseq man in black shirt and hat is sitting on bench endseq\n",
      "447111935_5af98563e3 startseq man in black shirt and hat is sitting on bench endseq\n",
      "3222041930_f642f49d28 startseq man in black shirt and hat is sitting on bench endseq\n",
      "2384353160_f395e9a54b startseq man in black shirt and hat is sitting on bench endseq\n",
      "1679617928_a73c1769be startseq man in black shirt and hat is sitting on bench endseq\n",
      "2693425189_47740c22ed startseq man in black shirt and hat is sitting on bench endseq\n",
      "3767841911_6678052eb6 startseq man in black shirt and hat is sitting on bench endseq\n",
      "1415591512_a84644750c startseq man in black shirt and hat is sitting on bench endseq\n",
      "534875358_6ea30d3091 startseq man in black shirt and hat is sitting on bench endseq\n",
      "2128119486_4407061c40 startseq man in black shirt and hat is sitting on bench endseq\n",
      "2797149878_bb8e27ecf9 startseq man in black shirt and hat is sitting on bench endseq\n",
      "3110649716_c17e14670e startseq man in black shirt and hat is sitting on bench endseq\n",
      "2378149488_648e5deeac startseq man in black shirt and hat is sitting on bench endseq\n",
      "434792818_56375e203f startseq man in black shirt and hat is sitting on bench endseq\n",
      "2288099178_41091aa00c startseq man in black shirt and hat is sitting on bench endseq\n",
      "2657484284_daa07a3a1b startseq man in black shirt and hat is sitting on bench endseq\n",
      "3239021459_a6b71bb400 startseq man in black shirt and hat is sitting on bench endseq\n",
      "300550441_f44ec3701a startseq man in black shirt and hat is sitting on bench endseq\n",
      "191003285_edd8d0cf58 startseq man in black shirt and hat is sitting on bench endseq\n",
      "3542484764_77d8920ec9 startseq man in black shirt and hat is sitting on bench endseq\n",
      "3396157719_6807d52a81 startseq man in black shirt and hat is sitting on bench endseq\n",
      "3262075846_5695021d84 startseq man in black shirt and hat is sitting on bench endseq\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-f7b3f36211e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model_19.h5'\u001b[0m \u001b[0;31m# 20 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_caption_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_feature_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-110-83c2dbd20dd3>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, y_caption_dict, y_feature_dict, tokenizer, maxlen)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_caption_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0m_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_feature_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# generate caption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcaption_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# collect answer captions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# collect prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-6f04a88c619b>\u001b[0m in \u001b[0;36mgenerate_caption\u001b[0;34m(model, tokenizer, image_feature, maxlen)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mseq_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mseq_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add padding to sequence vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mpred_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# predict next word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mpred_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_word\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# most probable word's index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx_to_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m       return training_arrays.predict_loop(\n\u001b[0;32m-> 1878\u001b[0;31m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, inputs, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.4/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load model and evaluate\n",
    "start = time.time()\n",
    "\n",
    "model_filename = 'model_19.h5' # 20 epochs\n",
    "model2 = load_model(model_filename)\n",
    "evaluate_model(model2, test_caption_dict, test_feature_dict, tokenizer, maxlen)\n",
    "\n",
    "end = time.time()\n",
    "print('Elapsed:', end-start, 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catpion generation example on test dataset\n",
    "start = time.time()\n",
    "\n",
    "max_example = [0, 0, 0] # [bleu_score, image_id, prediction]\n",
    "min_example = [1, 0, 0] # [bleu_score, image_id, prediction]\n",
    "for image_id in test_image_ids:\n",
    "    y = test_caption_dict[image_id]\n",
    "    y = [[x.split() for x in y]]\n",
    "    pred = generate_caption(model1, tokenizer, test_feature_dict[image_id], maxlen)\n",
    "    pred = [pred.split()]\n",
    "    bleu_score = corpus_bleu(y, pred, weights=(1.0, 0, 0, 0))\n",
    "    \n",
    "    if bleu_score > max_example[0]:\n",
    "        max_example[0] = bleu_score\n",
    "        max_example[1] = image_id\n",
    "        max_example[2] = pred\n",
    "        \n",
    "    if bleu_score < min_example[0]:\n",
    "        min_example[0] = bleu_score\n",
    "        min_example[1] = image_id\n",
    "        min_example[2] = pred\n",
    "    \n",
    "end = time.time()\n",
    "print('Elapsed:', end-start, 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('max bleu score:', max_example[0])\n",
    "image_filename = './Flicker8k_Dataset/' + max_example[1] + '.jpg'\n",
    "print(image_filename)\n",
    "for c in test_caption_dict[max_example[1]]:\n",
    "    print(' '.join(c.split()[1:-1]))\n",
    "\n",
    "display(Image(filename=image_filename))\n",
    "\n",
    "print(' '.join(max_example[2][0][1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('min bleu score:', min_example[0])\n",
    "image_filename = './Flicker8k_Dataset/' + min_example[1] + '.jpg'\n",
    "print(image_filename)\n",
    "for c in test_caption_dict[min_example[1]]:\n",
    "    print(' '.join(c.split()[1:-1]))\n",
    "\n",
    "display(Image(filename=image_filename))\n",
    "\n",
    "print(' '.join(min_example[2][0][1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
