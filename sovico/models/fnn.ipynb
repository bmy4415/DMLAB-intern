{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..') # to import preprocess.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.backend import tensorflow_backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from preprocess import initialize_logger\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y, mask):\n",
    "    '''\n",
    "    preprocess.prepate_data()의 결과를 FNN 직접 입력으로 넣을 수 있도록 처리해줌\n",
    "    NUM_EXAMPLES는 window를 적용한 후의 전체 TIME의 갯수이고\n",
    "    NUM_TIMES가 실제 train에 사용할 TIME의 갯수를 의미함\n",
    "    NUM_EXAMPLES와 NUM_TIMES가 다른 이유는 전체 NUM_EXAMPLES 중에서 label에 균형을 맞추어\n",
    "    일부 TIME의 일부 NODE에 대해서만 training을 진행하기 때문임\n",
    "    \n",
    "    FNN에서는 GCN과 달리 graph structure를 필요로하지 않으므로 mask를 이용하여 \n",
    "    x: (NUM_EXAMPLES, WINDOW_SIZE*8)\n",
    "    y: (NUM_EXAMPLES, NUM_CLASSES)\n",
    "    형태로만 전처리를 진행하면 됨\n",
    "    \n",
    "    :param: x (NUM_EXAMPLES, NUM_NODES, WINDOW_SIZE, 8)\n",
    "    :param: y (NUM_EXAMPLES, NUM_NODES\n",
    "    :param: mask (NUM_TIMES, NUM_NODES+1)\n",
    "    \n",
    "    :return (NUM_TIMES, x', y', mask')\n",
    "    x': numpy array of shape (NUM_TIMES, NUM_NODES, WINDOW_SIZE*8) which is flattened over WINDOW_SIZE\n",
    "    y': numpy array of shape (NUM_TIMES, NUM_NODES, NUM_CLASSES) which is one-hotted\n",
    "    '''\n",
    "    \n",
    "    x = np.reshape(x, [x.shape[0], x.shape[1], x.shape[2]*x.shape[3]])\n",
    "    y = to_categorical(y) # one-hot encoding\n",
    "    \n",
    "    x_ = []\n",
    "    y_ = []\n",
    "    \n",
    "    # this can be improved usnig 'numpytic' code\n",
    "    for i in range(mask.shape[0]):\n",
    "        time = mask[i, 0]\n",
    "        nodes = mask[i, 1:]\n",
    "        for j in range(nodes.shape[0]):\n",
    "            if nodes[j] == 1:\n",
    "                x_.append(x[time, j, :].tolist())\n",
    "                y_.append(y[time, j, :].tolist())\n",
    "    \n",
    "    x_ = np.array(x_) # shape: (NUM_EXAMPLES', WINDOW_SIZE*8)\n",
    "    y_ = np.array(y_) # shape: (NUM_EXAMPLES', NUM_CLASSES)\n",
    "    return x_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN():\n",
    "    '''\n",
    "    6-layer FNN network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, window_size, fnn_hiddens):\n",
    "        '''\n",
    "        FNN constructor\n",
    "        Each value in window is feature (8bit * window_size)\n",
    "        \n",
    "        :param: window_size number of frames to use\n",
    "        :param: fnn_hiddens list of length 6 which denotes neruons in each layer\n",
    "        '''\n",
    "\n",
    "        input_dim = window_size*8\n",
    "        num_classes = 4\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(fnn_hiddens[0], input_dim=input_dim))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Dense(fnn_hiddens[1]))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Dense(fnn_hiddens[2]))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Dense(fnn_hiddens[3]))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Dense(fnn_hiddens[4]))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Dense(fnn_hiddens[5]))\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Dense(num_classes)) # output layer\n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        self.model = model\n",
    "        \n",
    "    def fit(self, data, save_dir, learning_rate=0.0001, epochs=500, patience=30):\n",
    "        '''\n",
    "        train using given dataset and hyper-parameters and then save model to save_dir\n",
    "        because we use validation set, we will save model to save_dir everywhen there comes best validation loss\n",
    "        also we use early stopping with default patience as 20\n",
    "        (e.g. stop training if validation loss does not get better during consecutive 20 epochs)\n",
    "        \n",
    "        :param: data tuple of 3 dataset(train, valid, test) which is output of preprocess.prepare_data()\n",
    "        :param: save_dir path where trained model and train results will be saved\n",
    "        :param: learning_rate\n",
    "        :param: epochs\n",
    "        '''\n",
    "\n",
    "        logging.info('start fit function')\n",
    "        \n",
    "        # preprocess data for FNN\n",
    "        # x: (NUM_EXAMPLES, WINDOW_SIZE*8)\n",
    "        # y: (NUM_EXAMPLES, NUM_CLASSES)\n",
    "        train, valid, test = data\n",
    "        x_train, y_train = preprocess(*train)\n",
    "        x_valid, y_valid = preprocess(*valid)\n",
    "        x_test, y_test = preprocess(*test)\n",
    "        \n",
    "        assert x_train.shape[0] == y_train.shape[0]\n",
    "        assert x_valid.shape[0] == y_valid.shape[0]\n",
    "        assert x_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "        self.model.fit(x_train, y_train, epochs=epochs, batch_size=32,\n",
    "                       validation_data=(x_valid, y_valid), callbacks=[es],\n",
    "                       verbose=1)\n",
    "        \n",
    "        true_valid = np.argmax(y_valid, axis=1)\n",
    "        pred_valid = np.argmax(self.model.predict(x_valid), axis=1)\n",
    "        true_test = np.argmax(y_test, axis=1)\n",
    "        pred_test = np.argmax(self.model.predict(x_test), axis=1)\n",
    "        \n",
    "        acc_valid = accuracy_score(true_valid, pred_valid)\n",
    "        acc_test = accuracy_score(true_test, pred_test)\n",
    "\n",
    "        # valid set classification report\n",
    "        logging.info('valid acc: {}'.format(acc_valid))\n",
    "        logging.info(classification_report(true_valid, pred_valid))\n",
    "        # test set classification report\n",
    "        logging.info('test acc: {}'.format(acc_test))\n",
    "        logging.info(classification_report(true_test, pred_test))\n",
    "        \n",
    "        # save model\n",
    "        self.model.save(os.path.join(save_dir, 'model.h5'))\n",
    "        \n",
    "        # load model\n",
    "        # from keras.models import load_model\n",
    "        # model = load_model('my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modelname(window_size, after, fnn_hiddens):\n",
    "    '''\n",
    "    model을 식별할 수 있는 이름을 지정해주는 함수\n",
    "    '''\n",
    "    \n",
    "    return 'fnn_window{}_after{}_dims{}'.format(window_size, after, fnn_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, required=True, help='preprocessed data dir')\n",
    "    parser.add_argument('--window_size', type=int, required=True, help='number of frame in window')\n",
    "    parser.add_argument('--after', type=int, required=True, help='number of after window')\n",
    "    parser.add_argument('--save_dir', type=str, required=True, help='dir to save train result')\n",
    "    parser.add_argument('--lr', type=float, default=0.0001, help='learning_rate')\n",
    "    parser.add_argument('--fnn_hiddens', type=int, nargs='+')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # log와 ckpt가 저장될 directory 생성\n",
    "    modelname = get_modelname(args.window_size, args.after, args.fnn_hiddens)\n",
    "    path = os.path.join(args.save_dir, modelname)\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "    # logger 초기화\n",
    "    initialize_logger(path)\n",
    "        \n",
    "    # load data\n",
    "    data_filename = 'prediction_preprocessed_{}window_{}after.pkl'.format(args.window_size, args.after)\n",
    "    data_filename = os.path.join(args.data_dir, data_filename)\n",
    "    with open(data_filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    model = FNN(args.window_size, args.fnn_hiddens)\n",
    "    model.fit(data, save_dir=path, learning_rate=args.lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
