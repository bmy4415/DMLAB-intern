{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from preprocess import initialize_logger\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_matrix():\n",
    "    \"\"\"\n",
    "    Real map of 301 building\n",
    "    \"\"\"\n",
    "    arr = [\n",
    "        [0, 1, 0, 0],\n",
    "        [1, 0, 1, 0],\n",
    "        [0, 1, 0, 1],\n",
    "        [0, 0, 1, 0],\n",
    "    ]\n",
    "        \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(adj_matrix):\n",
    "    '''\n",
    "    return numpy array which represents normalized matrix, D^(-1/2) A D^(-1/2)\n",
    "    '''\n",
    "    ahat = np.array(adj_matrix) + np.eye(len(adj_matrix))\n",
    "    dhat_prime = np.diag(1 / np.sqrt(np.sum(ahat, axis=1)))\n",
    "    return np.matmul(np.matmul(dhat_prime, ahat), dhat_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y, mask):\n",
    "    '''\n",
    "    preprocess.prepate_data()의 결과를 LSTM_GCN에 직접 입력으로 넣을 수 있도록 처리해줌\n",
    "    NUM_EXAMPLES는 window를 적용한 후의 전체 TIME의 갯수이고\n",
    "    NUM_TIMES가 실제 train에 사용할 TIME의 갯수를 의미함\n",
    "    NUM_EXAMPLES와 NUM_TIMES가 다른 이유는 전체 NUM_EXAMPLES 중에서 label에 균형을 맞추어\n",
    "    일부 TIME의 일부 NODE에 대해서만 training을 진행하기 때문임\n",
    "    \n",
    "    :param: x (NUM_EXAMPLES, NUM_NODES, WINDOW_SIZE, 8)\n",
    "    :param: y (NUM_EXAMPLES, NUM_NODES)\n",
    "    :param: mask (NUM_TIMES, NUM_NODES+1)\n",
    "    \n",
    "    :return (x', y', mask')\n",
    "    x': numpy array of shape (NUM_TIMES, NUM_NODES, WINDOW_SIZE*8) which is flattened over WINDOW_SIZE\n",
    "    y': numpy array of shape (NUM_TIMES, NUM_NODES, NUM_CLASSES) which is one-hotted\n",
    "    mask': numpy array of shape (NUM_TIMES, NUM_NODES) which remove first element(TIME) in original mask\n",
    "    '''\n",
    "    \n",
    "    x_ = x\n",
    "    y_ = to_categorical(y) # one-hot encoding\n",
    "    \n",
    "    # apply times in mask\n",
    "    x_ = x_[mask[:, 0], :, :, :]\n",
    "    y_ = y_[mask[:, 0], :, :]\n",
    "    mask_ = mask[:, 1:]\n",
    "    \n",
    "    return x_, y_, mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_epoch(epoch, loss_train, true_train, pred_train, loss_valid, true_valid, pred_valid):\n",
    "    loss_tr = np.mean(loss_train)\n",
    "    acc_tr = accuracy_score(true_train, pred_train)\n",
    "    f1_tr = f1_score(true_train, pred_train, average='weighted')\n",
    "    loss_vd = np.mean(loss_valid)\n",
    "    acc_vd = accuracy_score(true_valid, pred_valid)\n",
    "    f1_vd = f1_score(true_valid, pred_valid, average='weighted')\n",
    "    logging.info('Epoch: %3d, loss_tr: %.4f, acc_tr: %.4f, f1_tr: %.4f, loss_vd: %.4f, acc_vd: %.4f, f1_vd: %.4f'\n",
    "         % (epoch+1, loss_tr, acc_tr, f1_tr, loss_vd, acc_vd, f1_vd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_GCN():\n",
    "    '''\n",
    "    LSTM + GCN model\n",
    "    window of frames is treated as sequence\n",
    "    each window is converted to vector via RNN network, also, this vector is feature of node\n",
    "    GCN is 2-layer\n",
    "    LSTM is 2-layer\n",
    "    '''\n",
    "    def __init__(self, window_size, adj_matrix, lstm_hiddens, gcn_hiddens):\n",
    "        '''\n",
    "        LSTM + GCN constructor\n",
    "        \n",
    "        :param: window_size number of frames in window\n",
    "        :param: adj_matrix NxN adjacent matrix with N nodes\n",
    "        :param: lstm_hiddens list of length 2 which denotes number of neurons in each lstm layer\n",
    "        :param: gcn_hiddens list of length 2 which denotes number of neurons in each gcn layer\n",
    "        '''\n",
    "        \n",
    "        N = len(adj_matrix) # number of nodes in graph\n",
    "        normalized_matrix = normalize_matrix(adj_matrix)\n",
    "        self.normalized_matrix = tf.constant(normalized_matrix, tf.float32)\n",
    "        \n",
    "        num_classes = 4\n",
    "        input_dim = 8 # 8bit sensor data\n",
    "        init = tf.initializers.he_normal()\n",
    "        \n",
    "        # placeholders\n",
    "        self.x = tf.placeholder(tf.float32, [N, window_size, input_dim])\n",
    "        self.y = tf.placeholder(tf.float32, [N, num_classes])\n",
    "        self.mask = tf.placeholder(tf.float32, [N])\n",
    "        \n",
    "        # define lstm cell\n",
    "        # using multirnn cell\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(hidden) for hidden in lstm_hiddens])\n",
    "                \n",
    "        # forward lstm, use tf.unstack for static_rnn\n",
    "        # outputs[-1] represents last hidden state of each batch\n",
    "        # in our case, shape: (batch_size=N, lstm_hiddens[-1])\n",
    "        outputs, _ = tf.nn.static_rnn(cell, tf.unstack(self.x, axis=1), dtype=tf.float32)\n",
    "        \n",
    "        # forward 2-layer GCN\n",
    "        self.W1 = tf.Variable(init([lstm_hiddens[-1], gcn_hiddens[0]]))\n",
    "        self.L1 = tf.matmul(self.normalized_matrix, outputs[-1])\n",
    "        self.L1 = tf.matmul(self.L1, self.W1)\n",
    "        self.L1 = tf.nn.tanh(self.L1)\n",
    "        \n",
    "        self.W2 = tf.Variable(init([gcn_hiddens[0], gcn_hiddens[1]]))\n",
    "        self.L2 = tf.matmul(self.normalized_matrix, self.L1)\n",
    "        self.L2 = tf.matmul(self.L2, self.W2)\n",
    "        self.L2 = tf.nn.tanh(self.L2)\n",
    "        \n",
    "        self.W3 = tf.Variable(init([gcn_hiddens[1], num_classes]))\n",
    "        self.L3 = tf.matmul(self.L2, self.W3)\n",
    "        \n",
    "        # loss\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.L3, labels=self.y)\n",
    "        self.loss = self.loss * self.mask # apply mask\n",
    "        self.loss = tf.reduce_mean(self.loss)\n",
    "        \n",
    "        # prediction\n",
    "        self.pred = tf.argmax(self.L3, axis=1)\n",
    "        \n",
    "    def fit(self, data, save_dir, learning_rate=0.001, epochs=500, patience=10):\n",
    "        '''\n",
    "        train using given dataset and hyper-parameters and then save model to save_dir\n",
    "        because we use validation set, we will save model to save_dir everywhen there comes best validation loss\n",
    "        also we use early stopping with default patience as 10\n",
    "        (e.g. stop training if validation loss does not get better during consecutive 20 epochs)\n",
    "        \n",
    "        :param: data tuple of 3 dataset(train, valid, test) which is output of preprocess.prepare_data()\n",
    "        :param: save_dir path where trained model and train results will be saved\n",
    "        :param: learning_rate\n",
    "        :param: epochs\n",
    "        '''\n",
    "\n",
    "        logging.info('start fit function')\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        train_step = optimizer.minimize(self.loss)\n",
    "        \n",
    "        y_true = tf.argmax(self.y, axis=1)\n",
    "        y_pred = tf.argmax(self.L3, axis=1)\n",
    "                \n",
    "        # preprocess data for LSTM_GCN\n",
    "        # x: (NUM_EXAMPLES, NUM_NODES, WINDOW_SIZE, 8)\n",
    "        # y: (NUM_EXAMPLES, NUM_NODES, NUM_CLASSES)\n",
    "        # mask: (NUM_EXAMPLES, NUM_NODES)\n",
    "        train, valid, test = data\n",
    "        x_train, y_train, mask_train = preprocess(*train)\n",
    "        x_valid, y_valid, mask_valid = preprocess(*valid)\n",
    "        x_test, y_test, mask_test = preprocess(*test)\n",
    "        \n",
    "        assert x_train.shape[0] == y_train.shape[0] == mask_train.shape[0]\n",
    "        assert x_valid.shape[0] == y_valid.shape[0] == mask_valid.shape[0]\n",
    "        assert x_test.shape[0] == y_test.shape[0] == mask_test.shape[0]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            min_valid_loss = None\n",
    "            patience_counter = 0\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                # shuffle train set\n",
    "                p = np.random.permutation(x_train.shape[0])\n",
    "                x_train_ = x_train[p]\n",
    "                y_train_ = y_train[p]\n",
    "                mask_train_ = mask_train[p]\n",
    "                \n",
    "                # iter over trainset\n",
    "                num_iter_train = x_train_.shape[0]\n",
    "                loss_train = []\n",
    "                true_train = []\n",
    "                pred_train = []\n",
    "                \n",
    "                for i in range(x_train_.shape[0]):\n",
    "                    l, true, pred, _ = sess.run([self.loss, y_true, y_pred, train_step], feed_dict={\n",
    "                        self.x: x_train_[i],\n",
    "                        self.y: y_train_[i],\n",
    "                        self.mask: mask_train_[i]\n",
    "                    })\n",
    "                    loss_train.append(l)\n",
    "                    true_train.extend(true[np.argwhere(mask_train_[i]).reshape(-1)].tolist())\n",
    "                    pred_train.extend(pred[np.argwhere(mask_train_[i]).reshape(-1)].tolist())                    \n",
    "                    \n",
    "                # iter over validset\n",
    "                num_iter_valid = x_valid.shape[0]\n",
    "                loss_valid = []\n",
    "                true_valid = []\n",
    "                pred_valid = []\n",
    "                \n",
    "                for i in range(x_valid.shape[0]):\n",
    "                    l, true, pred = sess.run([self.loss, y_true, y_pred], feed_dict={\n",
    "                        self.x: x_valid[i],\n",
    "                        self.y: y_valid[i],\n",
    "                        self.mask: mask_valid[i]\n",
    "                    })\n",
    "                    loss_valid.append(l)\n",
    "                    true_valid.extend(true[np.argwhere(mask_valid[i]).reshape(-1)].tolist())\n",
    "                    pred_valid.extend(pred[np.argwhere(mask_valid[i]).reshape(-1)].tolist())\n",
    "                    \n",
    "                # print this epoch\n",
    "                print_epoch(epoch, loss_train, true_train, pred_train, loss_valid, true_valid, pred_valid)\n",
    "                \n",
    "                # early stopping check\n",
    "                loss_valid = np.mean(loss_valid)\n",
    "                if min_valid_loss is None or min_valid_loss > loss_valid:\n",
    "                    min_valid_loss = loss_valid\n",
    "                    saver.save(sess, os.path.join(save_dir, 'model'))\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter > patience:\n",
    "                        break\n",
    "                               \n",
    "            # evaluation on testset\n",
    "            num_iter_test = x_test.shape[0]\n",
    "            loss_test = []\n",
    "            true_test = []\n",
    "            pred_test = []\n",
    "                \n",
    "            for i in range(num_iter_test):\n",
    "                l, true, pred = sess.run([self.loss, y_true, y_pred], feed_dict={\n",
    "                    self.x: x_test[i],\n",
    "                    self.y: y_test[i],\n",
    "                    self.mask: mask_test[i]\n",
    "                })\n",
    "                loss_test.append(l)\n",
    "                true_test.extend(true[np.argwhere(mask_test[i]).reshape(-1)].tolist())\n",
    "                pred_test.extend(pred[np.argwhere(mask_test[i]).reshape(-1)].tolist())\n",
    "                \n",
    "            loss_te = np.mean(loss_test)\n",
    "            acc_te = accuracy_score(true_test, pred_test)\n",
    "            f1_te = f1_score(true_test, pred_test, average='weighted')\n",
    "            logging.info('Evaluation, loss_te: %.4f, acc_te: %.4f, f1_te: %.4f' % (loss_te, acc_te, f1_te))\n",
    "            \n",
    "            # valid set classification report\n",
    "            logging.info(classification_report(true_valid, pred_valid))\n",
    "            # test set classification report\n",
    "            logging.info(classification_report(true_test, pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modelname(window_size, after, lstm_hiddens, gcn_hiddens):\n",
    "    '''\n",
    "    model을 식별할 수 있는 이름을 지정해주는 함수\n",
    "    '''\n",
    "    \n",
    "    return 'lstm_gcn_window{}_after{}_lstmdims{}_gcndims{}'.format(window_size, after, lstm_hiddens, gcn_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, required=True, help='preprocessed data path')\n",
    "    parser.add_argument('--window_size', type=int, required=True, help='number of frame in window')\n",
    "    parser.add_argument('--after', type=int, required=True, help='number of after window')\n",
    "    parser.add_argument('--save_dir', type=str, required=True, help='path to save train result')\n",
    "    parser.add_argument('--lr', type=str, default=0.0001, help='learning_rate')\n",
    "    parser.add_argument('--lstm_hiddens', type=int, nargs='+')\n",
    "    parser.add_argument('--gcn_hiddens', type=int, nargs='+')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # log와 ckpt가 저장될 directory 생성\n",
    "    modelname = get_modelname(args.window_size, args.after, args.lstm_hiddens, args.gcn_hiddens)\n",
    "    path = os.path.join(args.save_dir, modelname)\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    # logger 초기화\n",
    "    initialize_logger(path)\n",
    "\n",
    "    # load data\n",
    "    data_filename = 'prediction_preprocessed_{}window_{}after.pkl'.format(args.window_size, args.after)\n",
    "    data_filename = os.path.join(args.data_dir, data_filename)\n",
    "    with open(data_filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    adj_matrix = get_adj_matrix()\n",
    "    model = LSTM_GCN(args.window_size, adj_matrix, args.lstm_hiddens, args.gcn_hiddens)\n",
    "    model.fit(data, save_dir=path, learning_rate=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
