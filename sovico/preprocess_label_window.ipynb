{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_logger(log_dir):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # console logging\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"%(levelname)s - %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    \n",
    "    # file logging\n",
    "    handler = logging.FileHandler(os.path.join(log_dir, 'log'), 'w')\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"%(levelname)s - %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, NUM_NODES=8):\n",
    "    '''\n",
    "    Load text files in <data_path> and return numpy array of stacked sensor data.\n",
    "    Output np array has shape of (TOTAL_FRAMES, NUM_NODES, 9).\n",
    "    Last element of shape, that is 9, represents label of each frame(1bit) and sensor data(8bit).\n",
    "    Each label is digitized, which means, 0(0 person), 1(1-3 people), 2(4-6 people), 3(more than 7 people).\n",
    "    \n",
    "    For example, there are 8 nodes in 301 building and each node has 160000-320000 rows of 8bit sensor data.\n",
    "    Then, output np array will have shape of (160000, 8, 9)\n",
    "    \n",
    "    :param: data_path string which represents path of sensor data\n",
    "    :param: NUM_NODES integer which represents number of nodes\n",
    "    :return data numpy array of shape (TOTAL_FRAMES, NUM_NODES, 9)\n",
    "    '''\n",
    "    \n",
    "    data = []\n",
    "    for i in range(NUM_NODES):\n",
    "        curr = np.loadtxt(os.path.join(data_path, 'sensorData_{}.txt'.format(i+1)), delimiter=',', dtype=np.float32)\n",
    "        data.append(curr)\n",
    "\n",
    "    # data.shape: (TOTAL_FRAMES, NUM_NODES, 9)    \n",
    "    data = np.stack(data, axis=1)\n",
    "    \n",
    "    # apply label\n",
    "    bins=[1, 4, 7]\n",
    "    data[:, :, 0] = np.digitize(data[:, :, 0], bins)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_path, window_size, after, NUM_NODES=8):\n",
    "    '''\n",
    "    prediction task에서는 sensor data의 window를 사용하므로 window를 고려한 train, valid, test을 준비해야함\n",
    "    GCN을 이용할 것이므로 각각의 dataset은 서로 시간이 겹치면 안됨\n",
    "    또한 label이 매우 불균형 하므로 label을 기준으로 under-sampling을 진행한 후 train을 진행함\n",
    "    \n",
    "    - load_data 함수를 이용하여 data를 load함\n",
    "    - data: (TOTAL_FRAMES, NUM_NODES, 9)를 x: (TOTAL_FRAMES, NUM_NODES, 8), y: (TOTAL_FRAMES, num_nodes)로 분리함\n",
    "    - window를 적용하여 x: (NUM_EXAMPLES, NUM_NODES, 8), y: (NUM_EXAMPLES, NUM_NODES)로 변경\n",
    "    - time 기준으로 train, valid, test set으로 분리\n",
    "    - 각각의 dataset에서 label 숫자를 기준으로 (time, node_number)를 골라냄\n",
    "    - 골라낸 (time, node_number)를 time 기준으로 group화하여 time 마다 길이가 num_nodes인 mask vector를 만듬\n",
    "    \n",
    "    :param: data_path string which denotes raw sensor data path\n",
    "    :param: window_size integer which denotes window size and this should be multiple of 8\n",
    "    :param: after integer which denotes frame difference between window and label and this also should be multiple of 8\n",
    "    :param: NUM_NODES integer which denotes number of nodes\n",
    "    :return ((train_x, train_y, tarin_mask), (valid_x, valid_y, valid_mask), (test_x, test_y, test_mask))\n",
    "    '''\n",
    "    \n",
    "    def get_mask(y):\n",
    "        '''\n",
    "        return mask for prediction task\n",
    "        mask is 2d array and lenght of row is NUM_NODES+1\n",
    "        mask의 row의 첫번째 값은 time값이고 그 이후의 N개의 값은 해당 time에 있는 N개의 node 중에서 각각의 node를 loss를 계산할 때\n",
    "        이용할 것인지에 대한 0,1 값을 나타냄\n",
    "        예를들어 row: [126, 0, 1, 0, 0, 1] 인 경우, time=126인 시점에서 1, 4번 node만 loss 계산에 사용하겠다는 것을 의미함\n",
    "        \n",
    "        remind\n",
    "        이러한 mask를 이용하는 이유는 GCN의 input에 특정 time의 모든 node를 다 넣어야하고 동시에 똑같은 input을\n",
    "        FNN과 RNN에도 적용할 수 있어야 하기 때문임\n",
    "        \n",
    "        :param: y numpy array of shape (NUM_EXAMPLES, NUM_NODES) which denotes label of each sensor and this is not one-hot encoded\n",
    "        :return numpy 2d array of shape: (NUM_EXAMPLES, NUM_NODES+1) each row denotes [time, Node1, ... NodeN]\n",
    "        '''\n",
    "        \n",
    "        arr = [[], [], [], []]\n",
    "        y = y.tolist()\n",
    "        for t in range(len(y)):\n",
    "            for n in range(len(y[0])):\n",
    "                label = int(y[t][n])\n",
    "                arr[label].append((t, n))\n",
    "\n",
    "        # upto = min([len(x) for x in arr])\n",
    "        upto = 5000\n",
    "        result = []\n",
    "        for lst in arr:\n",
    "            random.seed(0)\n",
    "            random.shuffle(lst)\n",
    "            result.extend(lst[:upto])\n",
    "\n",
    "        # labels in original set\n",
    "        # for i in range(4):\n",
    "        #     print('label %d: %d' % (i, len(arr[i])))\n",
    "                        \n",
    "        # until now, result consists of (time, node_number)\n",
    "        # group by time\n",
    "        dic = dict()\n",
    "        for (t, n) in result:\n",
    "            if t in dic:\n",
    "                dic[t].append(n)\n",
    "            else:\n",
    "                dic[t] = [n]\n",
    "\n",
    "        # convert to row format: [t, n1, ... nn]\n",
    "        mask = []\n",
    "        for t in dic:\n",
    "            row = [t] + [0] * NUM_NODES\n",
    "            for n in dic[t]:\n",
    "                row[n+1] = 1\n",
    "                \n",
    "            mask.append(row)\n",
    "              \n",
    "        mask = np.array(mask)\n",
    "        mask = mask[mask[:, 0].argsort()] # sort by time asc\n",
    "        return mask\n",
    "    # end get_mask\n",
    "    \n",
    "    \n",
    "    # load data\n",
    "    data = load_data(data_path) # shape: (TIME, NUM_NODES, 9)\n",
    "    \n",
    "    # label window size: 1.5sec == 12 frame\n",
    "    LABEL_WINDOW_SIZE = 12\n",
    "    \n",
    "    # split into x, y and apply window\n",
    "    # this should be fixed to 'numpytic way' instead of for loop\n",
    "    x, y = [], []\n",
    "    for t in range(data.shape[0]-window_size-after):\n",
    "        curr_x = data[t:t+window_size, :, 1:] # shape: (window_size, NUM_NODES, 8)\n",
    "        curr_x = np.swapaxes(curr_x, 0, 1) # shape: (NUM_NODES, window_size, 8)\n",
    "        \n",
    "        # apply label window\n",
    "        y_start = t+window_size+after-1 - LABEL_WINDOW_SIZE\n",
    "        y_end = t+window_size+after + LABEL_WINDOW_SIZE\n",
    "        curr_y = data[y_start:y_end, :, 0] # shape: (LABEL_WINDOW_SIZE*2+1, NUM_NODES)\n",
    "        counts = np.apply_along_axis(\n",
    "            lambda x: np.bincount(x, minlength=4),\n",
    "            axis=0,\n",
    "            arr=curr_y.astype(np.int64))\n",
    "        curr_y = np.argmax(counts, axis=0)\n",
    "        \n",
    "        x.append(curr_x.tolist())\n",
    "        y.append(curr_y.tolist())\n",
    "        \n",
    "    x = np.array(x) # shape: (NUM_EXAMPLES, NUM_NODES, window_size, 8)\n",
    "    y = np.array(y) # shape: (NUM_EXAMPLES, NUM_NODES)\n",
    "    \n",
    "    # split into train/valid/test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # extract (t, n)s proportional to labels\n",
    "    mask_train = get_mask(y_train)\n",
    "    mask_valid = get_mask(y_valid)\n",
    "    mask_test = get_mask(y_test)\n",
    "    \n",
    "    return (x_train, y_train, mask_train), (x_valid, y_valid, mask_valid), (x_test, y_test, mask_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_path', type=str, required=True, help='sensor data path')\n",
    "    parser.add_argument('--window_size', type=int, required=True, help='number of frame in window')\n",
    "    parser.add_argument('--after', type=int, required=True, help='number of after window')\n",
    "    parser.add_argument('--save_path', type=str, required=True, help='save data path')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # main function\n",
    "    filename = 'prediction_preprocessed_{}window_{}after.pkl'.format(args.window_size, args.after)\n",
    "    path = os.path.join(args.save_path, filename)\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        print(path, 'already exists!')\n",
    "        \n",
    "    else:\n",
    "        print('start preprocess...')\n",
    "        \n",
    "        preprocessed = prepare_data(\n",
    "            data_path=args.data_path,\n",
    "            window_size=args.window_size,\n",
    "            after=args.after,\n",
    "            NUM_NODES=8\n",
    "        )\n",
    "        \n",
    "        # save as pickle file\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(preprocessed, f, protocol=4)\n",
    "            \n",
    "        print(path, 'saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
